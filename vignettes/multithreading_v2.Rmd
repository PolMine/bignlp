---
title: "Multithreading Reloaded"
author: "Andreas Blaette (andreas.blaette@uni-due.de)"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Multithreading Reloaded}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

## Loading packages

First thing in the morning and before we load the bignlp package, sufficient memory needs to be allocated for the JVM.

```{r}
options(java.parameters = "-Xmx4g")
```

```{r}
library(bignlp)
```

```{r}
library(pbapply)
library(polmineR)
library(data.table)
```


## Some data to process

```{r}
subcorpora <- corpus("GERMAPARLMINI") %>%
  subset(interjection == "speech") %>%
  split(s_attribute = "speaker")

speakers <- get_token_stream(subcorpora, collapse = " ", beautify = TRUE, verbose = FALSE)

dt <- data.table(id = 1L:length(speakers), text = unlist(speakers))
```



## Initialize StanfordCoreNLP

### Properties

```{r}
propsfile <- system.file(package = "bignlp", "extdata", "properties_files", "corenlp-german-fast.properties")
props <- properties(x = propsfile)
```

```{r}
properties_set_output_format(props, "json")
properties_set_threads(props, 7)
```


### Starting the engine

```{r}
SCNLP <- StanfordCoreNLP$new(properties = props, output_format = "json")
```

```{r}
SCNLP$verbose(FALSE)
```




## Parsing the text

### Step 1: Create chunkdirs

```{r}
nlpdir <- tempdir()
dirs <- mince(x = dt, dir = nlpdir, chunksize = 10L)
```


### Process files

```{r}
files <- pblapply(dirs, SCNLP$process_files)
```


### Parse JSON output

```{r}
json_files <- grep(".*\\.json", list.files(nlpdir, recursive = TRUE, full.names = TRUE), value = TRUE)
y <- pblapply(json_files, corenlp_parse_json, cl = 7L)
df <- do.call(rbind, y)
```


## Inspect result

```{r}
DT::datatable(df[1:1000,])
```