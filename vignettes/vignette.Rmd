---
title: "bignlp"
subtitle: "Annotate big corpora"
author: "Andreas Blaette (andreas.blaette@uni-due.de)"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{bignlp}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---
  
## The rationale of the bignlp-package

There are already a few packages for Natural Language Processing (NLP). These packages are not "pure R" NLP-tools. OpenNLP, coreNLP, or spaCy offer interfaces to standard NLP tools implemented in other programming languages. The cleanNLP R package manages to combine these external tools in one coherent framework. So why yet another NLP R package?

The existing packages are not good at dealing with large volumes of text. The thrust of the bignlp-package is to use a standard tool (Stanford CoreNLP) in parallel mode. To be parsimonious with the memory available, it implements  line-by-line processing, so that annotated data is not be kept in memory.

More technically speaking, there are three steps envisaged by in a bignlp workflow:

  1. Input data (XML, for instance) needs to be dissected into a two-column `data.table` with chunks of text (column "text") and a chunk id (column "id"). The purpose of the id is to serve as a link to relate chunks with metadata that is stored in another table (and that also has an id column).
  
  2. The input `data.table` is processed in single- or multi-threaded mode, using Stanford CoreNLP. The output of the `corenlp_annotate()`-function is written to one or several NDJSON files (NDJSON stands for newline-delimited JSON). Each line of the NDJSON files is a valid JSON string with the annotation data, and including the id.
  
  3. The NDJSON files are processed a line-by-line manner, resulting in a `data.table` with the chunk ids, and the tokenized and annotated text, of course.

Before we introduce code for realistic workflows using this approach, we explain the installation requirements.


## Installation

At this stage, the bignlp package uses [Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/). The CoreNLP code jar and models for specific languages can be downloaded from the project website without restrictions. It is important to have the correct version of Java installed. See the the Stanford CoreNLP project website for further details. To check which version R uses, use the follwing code:

```{r, eval = FALSE}
library(rJava)
.jinit()
.jcall("java/lang/System", "S", "getProperty", "java.runtime.version")
```

The easiest way to download Stanford CoreNLP is to (ab)use the installation mechanism included in the cleanNLP package. In the following example, we will annotate a few documents from the UN General Assembly corpus, so we need the English annotation tools.

Check the presence of the CoreNLP code jar as follows, and perform the download, if necessary.

```{r}
library(bignlp)
if (getOption("bignlp.corenlp_dir") == "") corenlp_install(type = "en")
```

Apart from the directory with the code jars, the location of a properties file to configure the annotators is required. Again, use the `system.file()`-function to find out where that is.

```{r}
options(
  bignlp.properties_file = system.file(package = "cleanNLP", "extdata", "StanfordCoreNLP-english-fast.properties")
)
```

Note that it is not necessary to use cleanNLP for downloading Stanford CoreNLP. CoreNLP and properties files can be stored anywhere on your system, and functions of the bignlp package take the paths as input.


## Preparations for text annotation

To be able to parallelize the annotation, *rJava may not be loaded* before initializing the CoreNLP annotator. This is a known [rJava issue](https://github.com/s-u/rJava/issues/25). It is important to consider that other packages may have instantiated a JVM already. The result may be that you cannot assign sufficient memory to the newly instantiated JVMs, the model cannot be loaded, and the process will fail.

There is an unexported function `rJava:::.check.JVM()` in the rJava package that you may use in an interactive session to check whether the JVM has been initialized already.

The JVMs initialized for tagging in parallel need a lot of memory. With 4GB, you are on the safe side. Set the memory limit for JVMs before doing anything else. 

```{r load_ctk, eval = FALSE}
options(java.parameters = "-Xmx4g") # needs to be set before a JVM is initialized.
noCores <- parallel::detectCores() - 2L
```

With bignlp, the logic is to write intermediate results to disk, so we do not have to keep everything in memory at a time. So we create the appropriate directory structure before anything else.

```{r}
outdir <- tempdir()
tsv_file <- file.path(outdir, "unga.tsv")
ndjson_file <- file.path(outdir, "unga.ndjson")
tsv_file_tagged <- file.path(outdir, "unga_tagged.tsv")
```

Finally, we load bignlp.

```{r}
packageVersion("bignlp")
```

The packages we will also need is the `data.table` package, mostly for its impressively `fread` and `fwrite` functions.

```{r}
library(data.table)
library(cwbtools)
```


## Example: Tagging the UNGA corpus

As a first step, we generate a tsv file with the text chunks that shall be processed.

```{r}
unga_xml_files <- list.files(
  system.file(package = "bignlp", "extdata", "xml"),
  full.names = TRUE
  )
CD <- CorpusData$new()
CD$import_xml(filenames = unga_xml_files)
fwrite(x = CD$chunktable, file = tsv_file)
```

Second, we call the tagger, producing NDJSON output.

```{r}
filenames_ndjson <- corenlp_annotate(x = tsv_file, destfile = ndjson_file)
```

The NDJSON output is parsed into a table ... 

```{r}
filenames_tsv_tagged <- corenlp_parse_ndjson(x = ndjson_file, destfile = tsv_file_tagged)
```

Finally, let us have a look at the table that is on disk.

```{r}
y <- data.table::fread(tsv_file_tagged)
```

This is exactly what we need to fill the `tokenstream`-slot of our CorpusData object.

The functions are designed to work in a pipe, so we can achieve the same result as follows:

```{r}
library(magrittr)
library(data.table)

dt <- corenlp_annotate(x = tsv_file, destfile = ndjson_file) %>% 
  corenlp_parse_ndjson(destfile = tsv_file_tagged) %>%
  lapply(fread) %>%
  rbindlist()
```



## Conclusions

Enjoy!

## Appendix: Installing CoreNLP

## Installing Stanford CoreNLP

A good and conventional place for installing a tool such as CoreNLP is the /opt dir. So from a terminal, create a directory for CoreNLP, go into it, download the zipped jar files, unzip it, and remove the zip file.

```{sh eval = FALSE}
mkdir /opt/stanford-corenlp
cd /opt/stanford-corenlp
wget http://nlp.stanford.edu/software/stanford-corenlp-full-2017-06-09.zip
unzip stanford-corenlp-full-2017-06-09.zip
rm stanford-corenlp-full-2017-06-09.zip
```

In the PolMine Project, we very often work with German data. So we illustrate getting models for a specific language for German. We go into the CoreNLP directory and download the model to this place.

```{sh, eval = FALSE}
cd stanford-corenlp-full-2017-06-09
wget http://nlp.stanford.edu/software/stanford-german-corenlp-2017-06-09-models.jar
```

The jar file with the model include a default properties file for processing German data. So if you want to configure the parser yourself, you may encounter the problem that this properties file included in the jar will override any other properties file you may want to use. One solution is to extract the properties file from the jar, remove it from the jar and edit it by hand to serve your needs.

```{sh, eval = FALSE}
unzip stanford-german-corenlp-2017-06-09-models.jar StanfordCoreNLP-german.properties
mv StanfordCoreNLP-german.properties ..
zip -d stanford-german-corenlp-2017-06-09-models.jar StanfordCoreNLP-german.properties
nano StanfordCoreNLP-german.properties 
```

Note that the bignlp package already includes a properties file that has been edited for annotating large amounts of data quickly. You will find it as follows:

```{r}
options(bignlp.properties_file = corenlp_get_properties_file(lang = "de", fast = TRUE))
```

Again, note that the properties file in the German model jar needs to be removed in the manner described before to make using the handcrafted properties file possible.

